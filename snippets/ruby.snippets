# block
snippet rr
	require_relative '${1}'
snippet .
	{|${2:v}| ${1}}
snippet ..
	{|${2:k,v}| ${1}}
snippet ...
	{|${2:a,b,c}| ${1}}
snippet beg
	begin
		${3}
	rescue ${1:Exception} => ${2:e}
	end

snippet begin
	begin
		raise 'A test exception.'
	rescue SystemExit, Interrupt
		raise
	rescue Exception => e
		puts e.message
		puts e.backtrace
	else
		# other exception
	ensure
		# always executed
	end

snippet csv_write
	require "csv"
	CSV.open("${path}", "wb") do |csv|
	  csv << ["row", "of", "CSV", "data"]
	end
snippet csv_read
	CSV.foreach(${1:file}, {:headers => true, :header_converters => :symbol}) do |row|
	end
snippet file_write
	File.open(${1:"path/to/file.dump"}, "wb") { |file| file.write() }
snippet singleton
	class Logger
		private_class_method :new
		@@logger = nil
		def Logger.create
			@@logger = new unless @@logger
			@@logger
		end
	end

	# from http://dalibornasevic.com/posts/9-ruby-singleton-pattern-again
	class Logger
	  def initialize
		@log = File.open("log.txt", "a")
	  end
	   
	  @@instance = Logger.new
	 
	  def self.instance
		return @@instance
	  end
	 
	  def log(msg)
		@log.puts(msg)
	  end
	 
	  private_class_method :new
	end
	 
	Logger.instance.log('message 1')

snippet md5
	require 'digest/md5'
	Digest::MD5.hexdigest("Hello World\n")
snippet new
	def initialize()
		${1}
	end
snippet utf8
	# encoding: UTF-8
snippet ei
	elsif $1

snippet Cached
	# cache something in a file
	# usage:
	#   cache = FileCached.new({:filename => "xx"})
	#   cache.with {|cached_thing| cached_thing[:key] = "value" unless cached_thing.include? :key }
	# the cache will be written automatically
	# TODO: make it threadsafe?
	class FileCached
		def initialize(config)
		@config = {
			:default => {},
			:filename => 'cache',
			:from_file => lambda{|filename| File.open(filename, "rb") { |file| Marshal.load(file) } },
			:to_binary => lambda{|t| Marshal.dump(t) },
				:load_cache  => true,
				:write_cache => true
			}.merge(config)
		end
	
		def load_cache
			return if @cache
			if @config[:load_cache] and (File.exist? @config[:filename])
				@cache = @config[:from_file].call(@config[:filename])
				@read  = @config[:to_binary].call(@cache)
			else
				@cache = @config[:default]
				@read = nil
			end
		end
	
		def dump_cache
			return unless @config[:write_cache]
			# check whether it was changed
			binary = @config[:to_binary].call(@cache)
			File.open(@config[:filename],"w") {|f| f.write binary } if binary != @read
		end
	
		def with(opts = {})
			load_cache
			begin
				r = yield @cache
			ensure
				dump_cache unless opts[:only_reading] or opts[:write_cache]
			end
			r
		end
	end
	
snippet Object_extend
	# some ideas taken from http://dannytatom.me/metaid/
	class Object
		# The hidden singleton lurks behind everyone
		def metaclass; class << self; self; end; end
		def meta_eval &blk; metaclass.instance_eval &blk; end
	
		# Adds methods to a metaclass
		def meta_def name, &blk
			meta_eval { define_method name, &blk }
		end
	
		# Defines an instance method within a class
		def class_def name, &blk
			class_eval { define_method name, &blk }
		end
	
		# prevent monkey_patching
		def def_soft (s_name, &blk)
			expect = Proc.new {|n|
				begin
					self.send(n, s_name)
					raise "bad #{s_name} already defined"
				rescue NameError
				end
			}
			expect.call (:public_instance_method)
			expect.call (:ic_instance_method)
			define_method(s_name, &blk)
		end
	

	end

snippet guard
	# encoding: utf-8
	# Guardfile
	
	guard :bundler do
		watch('Gemfile')
	end
	
	guard :rspec, :version => 2 do
		# run all specs if the spec_helper or supporting files files are modified
		watch('spec/spec_helper.rb')                      { 'spec' }
		watch(%r{\Aspec/(?:lib|support|shared)/.+\.rb\z}) { 'spec' }
	
		# run unit specs if associated lib code is modified
		watch(%r{\Alib/(.+)\.rb\z})                                         { |m| Dir["spec/unit/#{m[1]}"] }
		watch("lib/#{File.basename(File.expand_path('../', __FILE__))}.rb") { 'spec'                       }
	
		# run a spec if it is modified
		watch(%r{\Aspec/(?:unit|integration)/.+_spec\.rb\z})
	end

snippet d
	do
		${1}
	end

snippet d.
	do |${1}|
		${2}
	end
snippet d..
	do |${1},${2}|
		${3}
	end

snippet ext_array_products
	def products
	return [] if self.length == 0
	([self.first.map {|v| [v]}]
		.concat(self.drop(1))
	).reduce {|k,v| 
			puts "reducing #{k} #{v}"
			r = k.product(v).map {|k| 
		puts "k #{k}"
		k[0] + [k[1]] } 
			puts "r #{r}"
			r
	}
	end

snippet c
	class ${1}
	end

snippet pp
	require "pp"
	pp ${1}

snippet gemfile_tags
	group :tags do
		gem 'guard-ctags-bundler', :git => 'https://github.com/MarcWeber/guard-ctags-bundler.git'
	end

snippet gemfile_metrics
	group :metrics do
		gem 'flay',            '~> 1.4.2'
		gem 'flog',            '~> 2.5.1'
		gem 'reek',            '~> 1.2.8', :git => 'https://github.com/dkubb/reek.git'
		gem 'roodi',           '~> 2.1.0'
		gem 'yardstick',       '~> 0.5.0'
		gem 'yard-spellcheck', '~> 0.1.5'
		gem 'pelusa',          '~> 0.2.1'
	end
snippet freezer
	# see github.com/mbj/adamentium
	def define_memoize_method(method, freezer)
		original = instance_method(method)
		undef_method(method)
		define_method(method) do |*args|
	 memory.fetch(method) do
		 value  = original.bind(self).call(*args)
		 frozen = freezer.call(value)
		 store_memory(method, frozen)
	 end
		end
	end


snippet mechanize
	http://mechanize.rubyforge.org/EXAMPLES_rdoc.html
	require 'mechanize'
	
	a = Mechanize.new { |agent|
		agent.user_agent_alias = 'Mac Safari'
	}
	        
	a.get('http://google.com/') do |page|
			search_result = page.form_with(:name => 'f') do |search|
			search.q = 'Hello world'
		end.submit
		        
		search_result.links.each do |link|
			puts link.text
		end
	end
	a.history.clear
	
	# page = agent.parse(uri, resp, body)


snippet zlib
	Zlib::Inflate.inflate(Zlib::Deflate.deflate("abc", 9))

snippet mechanize_traverse
	def traverse(agent, cache, options)
	  require "addressable/uri"
	  require "set"
	
	  start_url = Addressable::URI.parse(options.fetch(:start_url))
	
	  # memorize visited urls
	  cache[:visited] ||= Set.new
	
	  cache[:bad] ||= Hash.new
	
	  cache[:skipped] ||= Set.new
	  cache[:skipped_rec_limit_reached] ||= Set.new
	  cache[:to_be_visited] ||= [[1, start_url.to_s]]
	
	  visited = cache[:visited]
	  bad = cache[:bad]
	  to_be_visited = cache[:to_be_visited]
	
	  follow_addressable = options.fetch(:follow_addressable,
	    lambda {|addressable| 
	    (addressable.host == start_url.host || addressable.host.nil? ) \
	    and ["", ".html", ".htm", ".php"].include? addressable.extname.downcase
	    }
	    )
	  visitor = options.fetch(:visitor, lambda {|uri, page, cache|})
	
	  max_recursion = options.fetch(:max_recursion, 5)
	
	  while to_be_visited.length > 0
	    rec, next_ = to_be_visited[0]
	    if rec > max_recursion
	      cache[:skipped_rec_limit_reached] << next_
	    else
	      cache[:skipped_rec_limit_reached].delete next_
	
	      next_uri = Addressable::URI.parse(next_)
	      if not visited.include? next_ and (not bad.include?(next_)) and follow_addressable.call(next_uri)
	        #debugging
	        begin
	          page = agent.get(next_)
	          cache[:visited] << next_
	          visitor.call(next_uri, page, cache)
	          refs = []
	          if (page.respond_to? :links)
	            page.links.to_a.each {|a|
	              href = a.attributes["href"]
	              if href
	                href.gsub!('#.*','')
	                # don't follow "." href values
	                # don't follow #foo href values
	                uri = Addressable::URI.parse(href)
	
	                # make absolute:
	                uri = next_uri + uri
	                # normalize query values:
	                uri.query_values = uri.query_values
	
	                if follow_addressable.call(uri) and not /^\.$|^#/ =~ href
	                  refs << [rec+1, uri.to_s] unless bad.include? href or visited.include? href
	                else
	                  cache[:skipped] << href
	                end
	              end
	            }
	            to_be_visited.concat(refs)
	            cache[next_] = refs
	          end
	        rescue Mechanize::ResponseCodeError, Mechanize::UnsupportedSchemeError, OpenSSL::SSL::SSLError => e
	          bad[next_] =e.to_s
	        end
	        visited << next_
	      end
	    end
	    to_be_visited.delete_at(0)
	  end
	
	end

snippet imap_examine
	def examine(user, password, host, sep, port, ssl) 
	  folders = Hash.new
	  ssl = ssl.nil? ? false : ssl
	  opts = {}
	  opts[:ssl] = {:verify_mode=> OpenSSL::SSL::VERIFY_NONE} if ssl
	  opts[:port] = port unless port.nil?
	  imap = Net::IMAP.new(host, opts)
	  begin
	    imap.login( user, password)
	    total = 0
	    recurse_folders(imap, nil, sep, lambda {|foldername|
	      # puts "counting in #{foldername} #{imap}"
	      #debugging
	      count = -1
	      begin
	        count = imap.search(["ALL"]).length
	      rescue Exception => e
	        # puts "failed retrieving emails #{foldername}"
	      end
	
	      foldername.gsub!(/^INBOX\./,'')
	      foldername.gsub!(/^INBOX$/,'Inbox')
	      folders[foldername] = count
	      total += count
	    })
	    folders["total"] = total
	  rescue Exception => e
	    puts e.message
	    puts e.backtrace
	  ensure
	    imap.logout
	    imap.disconnect
	  end
	  folders
	end

snippet format
	05d" % v

snippet SlowValue
	require "thread"
	
	$mutex = Mutex.new
	class Workers
	
	  # from https://github.com/grosser/parallel/blob/master/lib/parallel.rb#L63
	  def processor_count
	    @processor_count ||= case RbConfig::CONFIG['host_os']
	    when /darwin9/
	      `hwprefs cpu_count`.to_i
	    when /darwin/
	      (hwprefs_available? ? `hwprefs thread_count` : `sysctl -n hw.ncpu`).to_i
	    when /linux|cygwin/
	      `grep -c ^processor /proc/cpuinfo`.to_i
	    when /(open|free)bsd/
	      `sysctl -n hw.ncpu`.to_i
	    when /mswin|mingw/
	      require 'win32ole'
	      wmi = WIN32OLE.connect("winmgmts://")
	      cpu = wmi.ExecQuery("select NumberOfLogicalProcessors from Win32_Processor")
	      cpu.to_enum.first.NumberOfLogicalProcessors
	    when /solaris2/
	      `psrinfo -p`.to_i # this is physical cpus afaik
	    else
	      $stderr.puts "Unknown architecture ( #{RbConfig::CONFIG["host_os"]} ) assuming one processor."
	      1
	    end
	  end
	
	  def initialize()
	    @job_queue = Queue.new
	    for i in 0..processor_count
	      Thread.new do
	        @job_queue.deq.call
	      end
	    end
	  end
	
	  def do_work(&blk)
	    @job_queue.enq (blk)
	  end
	
	  @@instance = Workers.new
	
	end
	
	class SlowValue
	
	  def initialize(&blk)
	    @blk = blk
	  end
	
	  def compute
	    $mutex.synchronize do
	      return if defined? @result_queue
	      @result_queue = Queue.new
	      Workers.instance.do_work do
	        @result_queue.enq(@blk.call)
	      end
	    end
	    self
	  end
	
	  def value
	    compute
	    $mutex.synchronize {
	      @result ||= @result_queue.deq
	    }
	    return @result
	  end
	
	  def method_missing(meth, *args, &blk)
	    value.__send(meth, *args, &blk)
	  end
	
	  def to_s
	    value.to_s
	  end
	
	  def ==(b)
	    value == b
	  end
	
	end
	
	slow_big_endian_test = SlowValue.new do
	  sleep 3
	  "#define HAS_BIG_ENDIAN"
	end
	
	has_header_x = SlowValue.new($workers) do
	  sleep 3
	  "#define HAS_HEADER_X"
	end
	
	config_h = []
	config_h << slow_big_endian_test.compute
	config_h << has_header_x.compute
	
	puts config_h
	
	class Logger
	  private_class_method :new
	  @@logger = nil
	  def Logger.create
	    @@logger = new unless @@logger
	    @@logger
	  end
	end

snippet mail
	require 'mail'
	Mail.deliver do
	   from     'me@test.lindsaar.net'
	   to       'you@test.lindsaar.net'
	   subject  'Here is the image you wanted'
	   body     File.read('body.txt')
	   add_file '/full/path/to/somefile.png'
	end

snippet thread_abort_on_exception
	Thread.abort_on_exception = true

snippet google_lat_lng
	def google_lat_lon(address)
	  require "addressable/uri"
	  require 'net/http'
	  url = Addressable::URI.parse('http://maps.google.com/maps/api/geocode/json?&sensor=false&region=de')
	  url.query_values = url.query_values.merge({:address => address})
	  req = Net::HTTP::Get.new(url.request_uri)
	  res = Net::HTTP.start(url.host, url.port) {|http|
	    http.request(req)
	  }
	  j = JSON.parse(res.body)
	  raise 'google query limit exceeded' if j[:status] && j[:status] == 'OVER_QUERY_LIMIT'
	  j.fetch("results", []).fetch(0, {}).fetch("geometry", {}).fetch("location", nil)
	end
snippet sequel
	require 'sequel'
	# mysql2 for utf-8
	DB = Sequel.connect('mysql2://user:pass@host/databasename')
	# DB.loggers << Logger.new($stdout)
	DB[:table].where(x => 7).first/all/update/delete

snippet me
	require 'mechanize'
	a = Mechanize.new

snippet dag
	module DAG
		class Node
			attr_reader :childs
			attr_reader :value
			def initialize(value, childs = [])
				@value = value
				@childs = childs
			end

			def to_string(indent = 0)
				s = ""
				s += "	" * indent + @value.to_s + "\n"
				@childs.each do |c|
					s += c.to_string(indent+1)
				end
				s
			end
		end

		class DAG
			attr_reader :root_nodes

			def initialize(*root_nodes)
				@root_nodes = root_nodes
			end

			def to_string(indent = 0)
				s = ""
				root_nodes.each do |node|
					s += node.to_string(indent)+"\n"
				end
				s
			end

			def self.from_vertices(*vertices)
				roots = Set.new
				content_to_node_object = Hash.new {|h,t|
					n = Node.new(t)
					roots << n
					h[t] = n
					h[t]
				}

				i = 0
				while i < vertices.length
					from = content_to_node_object[vertices[i]]
					to = content_to_node_object[vertices[i+1]]
					roots.delete to
					from.childs << to
					i+=2
				end

				DAG.new(*roots.to_a)
			end
		end
	end
snippet try_times
	def try_times(num)
		ex = nil
		0.upto(num).each do |v|
			begin
				return yield
			rescue SystemExit, Interrupt
				raise
			rescue Exception => e
				ex = e
				puts "retrying"
			end
		end
		raise ex
	end

snippet steps_runner
	class StepsRunner
		def initialize(o)
			@steps			 = o.fetch(:steps)
			@num_threads = o.fetch(:num_threads, 1)
		end
	
		def run_all
			@steps.each {|v| run v}
		end
	
		def results_of(step_name, opts)
			add_numbers = opts.fetch(:add_numbers)
			$DB[:tasks].where(:step_name => step_name).select(:result).map do |v|
				result = v[:result] 
				result.add_numbers if opts[:add_numbers]
			end
		end
	
		def run(step)
			step_name = step.fetch(:step_name)
			puts "running #{step_name}, updating tasks .."
	
			inputs = {}
			step.fetch(:inputs, []).map { |name| 
				inputs[name] = results_of(name)
			}
	
			step[:generate].call(inputs).each_slice(100).each do |tasks|
				$DB[:tasks].insert_conflict.multi_insert(tasks.map {|task| {:step_name => step_name}.merge(:task => Sequel.pg_jsonb_wrap(task)) } )
			end
			puts "done, getting tasks without results: "
	
			tasks = $DB[:tasks].where(:step_name => step_name).where( :result => nil).where(:exception => nil).all
	
			puts "done, task count: #{tasks.length}, starting work using #{@num_threads} threads"
	
			m = Mutex.new
	
			loop = lambda do
				while true
					task = m.synchronize { tasks.shift }
					puts "task #{task.inspect}"
					break if task.nil?
					up = $DB[:tasks].where(:id =>task[:id])
					result = begin
										 r = step[:run].call(task[:task])
										 m.synchronize { up.update(:result => Sequel.pg_jsonb_wrap(r)) }
									 rescue Exception => e
										 # r = step[:run].call(task[:task])
										 # m.synchronize { up.update(:result => Sequel.pg_jsonb_wrap(r)) }
	
										 puts e.message
										 puts e.backtrace.inspect
										 m.synchronize { up.update(:exception => e.message) }
										 e.message
									 end
					puts "#{step_name}: #{task.inspect} -> #{result.inspect}"
				end
			end
	
			case @num_threads
			when 1
				loop.call
			else
				threads = []
				1.upto(@num_threads).each do |v|
					threads << Thread.new do
						loop.call
					end
				end
	
				threads.each do |thread|
					thread.join
				end
			end
			puts "#{step_name} done"
		end
	end
snippet ep
	eachpair do |k,v|
	end
snippet ev
	each do |v|
	end
